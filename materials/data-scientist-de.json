{
  "profession": "data-scientist",
  "language": "de",
  "categories": [
    {
      "id": "interview-questions",
      "name": "Vorstellungsgespr√§ch Fragen",
      "count": 156,
      "icon": "üí¨",
      "color": "bg-blue-100 text-blue-800"
    },
    {
      "id": "technical-tasks",
      "name": "Technische Aufgaben",
      "count": 89,
      "icon": "‚ö°",
      "color": "bg-green-100 text-green-800"
    },
    {
      "id": "system-design",
      "name": "System Design",
      "count": 45,
      "icon": "üèóÔ∏è",
      "color": "bg-purple-100 text-purple-800"
    },
    {
      "id": "behavioral",
      "name": "Verhaltensfragen",
      "count": 67,
      "icon": "üß†",
      "color": "bg-orange-100 text-orange-800"
    },
    {
      "id": "algorithms",
      "name": "Algorithmen & Datenstrukturen",
      "count": 123,
      "icon": "üìä",
      "color": "bg-red-100 text-red-800"
    },
    {
      "id": "best-practices",
      "name": "Best Practices",
      "count": 78,
      "icon": "‚≠ê",
      "color": "bg-yellow-100 text-yellow-800"
    }
  ],
  "materials": [
    {
      "id": 1,
      "title": "Maschinelles Lernen: Von Grundlagen zu fortgeschrittenen Algorithmen",
      "description": "Umfassender Leitfaden zum maschinellen Lernen mit praktischen Beispielen",
      "category": "interview-questions",
      "difficulty": "advanced",
      "readTime": 30,
      "rating": 4.9,
      "reads": 1345,
      "tags": ["Maschinelles Lernen", "Python", "Scikit-learn", "Algorithmen"],
      "content": "# Maschinelles Lernen: Von Grundlagen zu fortgeschrittenen Algorithmen\n\n## √úberwachtes Lernen\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Datenvorbereitung\nX = df[['merkmal1', 'merkmal2', 'merkmal3']]\ny = df['ziel']\n\n# Datenaufteilung\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Modelltraining\nmodell = RandomForestClassifier(n_estimators=100, random_state=42)\nmodell.fit(X_train, y_train)\n\n# Vorhersagen\ny_pred = modell.predict(X_test)\nprint(classification_report(y_test, y_pred))\n```\n\n## Deep Learning\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# Modell erstellen\nmodell = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\n# Kompilieren\nmodell.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Training\nverlauf = modell.fit(X_train, y_train, epochs=50, validation_split=0.2)\n```\n\n## Feature Engineering\n\n```python\n# Neue Features erstellen\ndf['merkmal_verhaeltnis'] = df['merkmal1'] / df['merkmal2']\ndf['merkmal_summe'] = df['merkmal1'] + df['merkmal2']\n\n# Polynomielle Features\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X_scaled)\n```\n\n## Modellvalidierung\n\n```python\nfrom sklearn.model_selection import cross_val_score\n\n# Kreuzvalidierung\ncv_scores = cross_val_score(modell, X, y, cv=5, scoring='accuracy')\nprint(f'Durchschnittlicher CV-Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})')\n```\n\n## Fazit\n\nMaschinelles Lernen erfordert tiefes Verst√§ndnis von Algorithmen, Datenmanipulationsf√§higkeiten und kontinuierliches Lernen neuer Methoden.",
      "isNew": false,
      "isPopular": true,
      "createdAt": "2024-01-01T00:00:00Z"
    },
    {
      "id": 2,
      "title": "Deep Learning mit TensorFlow und PyTorch",
      "description": "Praktischer Leitfaden zum Deep Learning",
      "category": "technical-tasks",
      "difficulty": "advanced",
      "readTime": 25,
      "rating": 4.8,
      "reads": 987,
      "tags": ["Deep Learning", "TensorFlow", "PyTorch", "Neuronale Netze"],
      "content": "# Deep Learning mit TensorFlow und PyTorch\n\n## TensorFlow\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Funktionale API\ninputs = tf.keras.Input(shape=(784,))\nx = layers.Dense(128, activation='relu')(inputs)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(10, activation='softmax')(x)\n\nmodell = tf.keras.Model(inputs=inputs, outputs=outputs)\nmodell.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n```\n\n## PyTorch\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass NeuronalesNetz(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.schicht1 = nn.Linear(input_size, hidden_size)\n        self.schicht2 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        x = self.relu(self.schicht1(x))\n        x = self.schicht2(x)\n        return x\n\nmodell = NeuronalesNetz(784, 128, 10)\nkriterium = nn.CrossEntropyLoss()\noptimierer = torch.optim.Adam(modell.parameters())\n```\n\n## Fazit\n\nDeep Learning er√∂ffnet neue M√∂glichkeiten zur L√∂sung komplexer Probleme, erfordert aber erhebliche Rechenressourcen.",
      "isNew": true,
      "isPopular": true,
      "createdAt": "2024-01-15T00:00:00Z"
    },
    {
      "id": 3,
      "title": "Verarbeitung nat√ºrlicher Sprache (NLP)",
      "description": "Moderne Methoden der Textverarbeitung und Sprachmodelle",
      "category": "best-practices",
      "difficulty": "advanced",
      "readTime": 22,
      "rating": 4.7,
      "reads": 756,
      "tags": ["NLP", "BERT", "Transformers", "Tokenisierung"],
      "content": "# Verarbeitung nat√ºrlicher Sprache (NLP)\n\n## Transformer\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# Vorab trainierte Modelle laden\nmodell_name = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(modell_name)\nmodell = AutoModel.from_pretrained(modell_name)\n\n# Text tokenisieren\ntext = \"Hallo, wie geht es dir?\"\ninputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n\n# Embeddings erhalten\nwith torch.no_grad():\n    outputs = modell(**inputs)\n    embeddings = outputs.last_hidden_state\n```\n\n## Fine-tuning\n\n```python\nfrom transformers import Trainer, TrainingArguments\n\n# Trainingsargumente\ntraining_args = TrainingArguments(\n    output_dir='./ergebnisse',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n)\n\n# Training\ntrainer = Trainer(model=modell, args=training_args, train_dataset=train_dataset)\ntrainer.train()\n```\n\n## Fazit\n\nNLP entwickelt sich schnell dank Transformer und gro√üen Sprachmodellen. Es ist wichtig, mit neuen Entwicklungen Schritt zu halten.",
      "isNew": true,
      "isPopular": false,
      "createdAt": "2024-01-20T00:00:00Z"
    }
  ]
}
