{
  "profession": "data-scientist",
  "language": "ru",
  "categories": [
    {
      "id": "interview-questions",
      "name": "–í–æ–ø—Ä–æ—Å—ã –Ω–∞ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–∏",
      "count": 156,
      "icon": "üí¨",
      "color": "bg-blue-100 text-blue-800"
    },
    {
      "id": "technical-tasks",
      "name": "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è",
      "count": 89,
      "icon": "‚ö°",
      "color": "bg-green-100 text-green-800"
    },
    {
      "id": "system-design",
      "name": "–°–∏—Å—Ç–µ–º–Ω—ã–π –¥–∏–∑–∞–π–Ω",
      "count": 45,
      "icon": "üèóÔ∏è",
      "color": "bg-purple-100 text-purple-800"
    },
    {
      "id": "behavioral",
      "name": "–ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã",
      "count": 67,
      "icon": "üß†",
      "color": "bg-orange-100 text-orange-800"
    },
    {
      "id": "algorithms",
      "name": "–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö",
      "count": 123,
      "icon": "üìä",
      "color": "bg-red-100 text-red-800"
    },
    {
      "id": "best-practices",
      "name": "–õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏",
      "count": 78,
      "icon": "‚≠ê",
      "color": "bg-yellow-100 text-yellow-800"
    }
  ],
  "materials": [
    {
      "id": 1,
      "title": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –æ—Ç –æ—Å–Ω–æ–≤ –¥–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤",
      "description": "–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏",
      "category": "interview-questions",
      "difficulty": "advanced",
      "readTime": 30,
      "rating": 4.9,
      "reads": 1345,
      "tags": ["–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "Python", "Scikit-learn", "–ê–ª–≥–æ—Ä–∏—Ç–º—ã"],
      "content": "# –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –æ—Ç –æ—Å–Ω–æ–≤ –¥–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤\n\n## –û—Å–Ω–æ–≤—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n\n### –¢–∏–ø—ã –æ–±—É—á–µ–Ω–∏—è\n\n```python\n# 1. –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (Supervised Learning)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\nX = df[['feature1', 'feature2', 'feature3']]\ny = df['target']\n\n# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\npredictions = model.predict(X_test)\n\n# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\nmse = mean_squared_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(f'MSE: {mse:.4f}')\nprint(f'R¬≤: {r2:.4f}')\n```\n\n### –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\ny_pred = clf.predict(X_test)\n\n# –û—Ü–µ–Ω–∫–∞\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': clf.feature_importances_\n}).sort_values('importance', ascending=False)\n```\n\n## –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã\n\n### –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥\n\n```python\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\n# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ XGBoost\nxgb_model = xgb.XGBClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=6,\n    random_state=42\n)\n\n# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 6, 9],\n    'learning_rate': [0.01, 0.1, 0.2]\n}\n\n# –ü–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\ngrid_search = GridSearchCV(\n    xgb_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1\n)\ngrid_search.fit(X_train, y_train)\n\nprint(f'–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}')\nprint(f'–õ—É—á—à–∏–π —Å–∫–æ—Ä: {grid_search.best_score_:.4f}')\n```\n\n### –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\n# –ö–æ–º–ø–∏–ª—è—Ü–∏—è\nmodel.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# –û–±—É—á–µ–Ω–∏–µ\nhistory = model.fit(\n    X_train, y_train,\n    epochs=50,\n    batch_size=32,\n    validation_split=0.2,\n    verbose=1\n)\n```\n\n## –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n\n### –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞\n\n```python\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\nimputer = SimpleImputer(strategy='median')\nX_imputed = imputer.fit_transform(X)\n\n# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_imputed)\n\n# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\nle = LabelEncoder()\ncategorical_columns = ['category1', 'category2']\nfor col in categorical_columns:\n    df[col] = le.fit_transform(df[col])\n```\n\n### Feature Engineering\n\n```python\n# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\ndf['feature_ratio'] = df['feature1'] / df['feature2']\ndf['feature_sum'] = df['feature1'] + df['feature2']\ndf['feature_product'] = df['feature1'] * df['feature2']\n\n# –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X_scaled)\n```\n\n## –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π\n\n```python\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n\n# –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n\nprint(f'–°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})')\n\n# –ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(\n    model, X, y, cv=cv, n_jobs=-1,\n    train_sizes=np.linspace(0.1, 1.0, 10)\n)\n\nplt.figure(figsize=(10, 6))\nplt.plot(train_sizes, train_scores.mean(axis=1), label='–û–±—É—á–∞—é—â–∏–π —Å–∫–æ—Ä')\nplt.plot(train_sizes, val_scores.mean(axis=1), label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–∫–æ—Ä')\nplt.xlabel('–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏')\nplt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')\nplt.legend()\nplt.show()\n```\n\n## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n\n–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, —É–º–µ–Ω–∏—è —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–æ–≤—ã–º –º–µ—Ç–æ–¥–∞–º. –í–∞–∂–Ω–æ –Ω–µ —Ç–æ–ª—å–∫–æ —É–º–µ—Ç—å –ø—Ä–∏–º–µ–Ω—è—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º—ã, –Ω–æ –∏ –ø–æ–Ω–∏–º–∞—Ç—å –∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã.",
      "isNew": false,
      "isPopular": true,
      "createdAt": "2024-01-01T00:00:00Z"
    },
    {
      "id": 2,
      "title": "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å TensorFlow –∏ PyTorch",
      "description": "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –≥–ª—É–±–æ–∫–æ–º—É –æ–±—É—á–µ–Ω–∏—é",
      "category": "technical-tasks",
      "difficulty": "advanced",
      "readTime": 25,
      "rating": 4.8,
      "reads": 987,
      "tags": ["–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "TensorFlow", "PyTorch", "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏"],
      "content": "# –ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å TensorFlow –∏ PyTorch\n\n## TensorFlow\n\n### –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\n# –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π API\ninputs = tf.keras.Input(shape=(784,))\nx = layers.Dense(128, activation='relu')(inputs)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(64, activation='relu')(x)\noutputs = layers.Dense(10, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n# –ö–æ–º–ø–∏–ª—è—Ü–∏—è\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n```\n\n### –û–±—É—á–µ–Ω–∏–µ\n\n```python\n# Callbacks\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3),\n    tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)\n]\n\n# –û–±—É—á–µ–Ω–∏–µ\nhistory = model.fit(\n    X_train, y_train,\n    epochs=100,\n    batch_size=32,\n    validation_data=(X_val, y_val),\n    callbacks=callbacks\n)\n```\n\n## PyTorch\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(input_size, hidden_size)\n        self.layer2 = nn.Linear(hidden_size, hidden_size)\n        self.layer3 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, x):\n        x = self.dropout(self.relu(self.layer1(x)))\n        x = self.dropout(self.relu(self.layer2(x)))\n        x = self.layer3(x)\n        return x\n\n# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\nmodel = NeuralNetwork(input_size=784, hidden_size=128, output_size=10)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n```\n\n## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n\n–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.",
      "isNew": true,
      "isPopular": true,
      "createdAt": "2024-01-15T00:00:00Z"
    },
    {
      "id": 3,
      "title": "–û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP)",
      "description": "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
      "category": "best-practices",
      "difficulty": "advanced",
      "readTime": 22,
      "rating": 4.7,
      "reads": 756,
      "tags": ["NLP", "BERT", "Transformers", "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"],
      "content": "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP)\n\n## –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã\n\n### –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\nmodel_name = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\ntext = \"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\"\ninputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n\n# –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\nwith torch.no_grad():\n    outputs = model(**inputs)\n    embeddings = outputs.last_hidden_state\n```\n\n### Fine-tuning\n\n```python\nfrom transformers import Trainer, TrainingArguments\n\n# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\ntrain_dataset = prepare_dataset(train_texts, train_labels)\nval_dataset = prepare_dataset(val_texts, val_labels)\n\n# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n)\n\n# –û–±—É—á–µ–Ω–∏–µ\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\ntrainer.train()\n```\n\n## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n\nNLP –±—ã—Å—Ç—Ä–æ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è –±–ª–∞–≥–æ–¥–∞—Ä—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º –∏ –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º. –í–∞–∂–Ω–æ —Å–ª–µ–¥–∏—Ç—å –∑–∞ –Ω–æ–≤—ã–º–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞–º–∏ –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.",
      "isNew": true,
      "isPopular": false,
      "createdAt": "2024-01-20T00:00:00Z"
    }
  ]
}
