{
  "profession": "data-scientist",
  "language": "zh",
  "categories": [
    {
      "id": "interview-questions",
      "name": "é¢è¯•é—®é¢˜",
      "count": 156,
      "icon": "ğŸ’¬",
      "color": "bg-blue-100 text-blue-800"
    },
    {
      "id": "technical-tasks",
      "name": "æŠ€æœ¯ä»»åŠ¡",
      "count": 89,
      "icon": "âš¡",
      "color": "bg-green-100 text-green-800"
    },
    {
      "id": "system-design",
      "name": "ç³»ç»Ÿè®¾è®¡",
      "count": 45,
      "icon": "ğŸ—ï¸",
      "color": "bg-purple-100 text-purple-800"
    },
    {
      "id": "behavioral",
      "name": "è¡Œä¸ºé—®é¢˜",
      "count": 67,
      "icon": "ğŸ§ ",
      "color": "bg-orange-100 text-orange-800"
    },
    {
      "id": "algorithms",
      "name": "ç®—æ³•ä¸æ•°æ®ç»“æ„",
      "count": 123,
      "icon": "ğŸ“Š",
      "color": "bg-red-100 text-red-800"
    },
    {
      "id": "best-practices",
      "name": "æœ€ä½³å®è·µ",
      "count": 78,
      "icon": "â­",
      "color": "bg-yellow-100 text-yellow-800"
    }
  ],
  "materials": [
    {
      "id": 1,
      "title": "æœºå™¨å­¦ä¹ ï¼šä»åŸºç¡€åˆ°é«˜çº§ç®—æ³•",
      "description": "åŒ…å«å®ç”¨ç¤ºä¾‹çš„æœºå™¨å­¦ä¹ ç»¼åˆæŒ‡å—",
      "category": "interview-questions",
      "difficulty": "advanced",
      "readTime": 30,
      "rating": 4.9,
      "reads": 1345,
      "tags": ["æœºå™¨å­¦ä¹ ", "Python", "Scikit-learn", "ç®—æ³•"],
      "content": "# æœºå™¨å­¦ä¹ ï¼šä»åŸºç¡€åˆ°é«˜çº§ç®—æ³•\n\n## ç›‘ç£å­¦ä¹ \n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# æ•°æ®å‡†å¤‡\nX = df[['ç‰¹å¾1', 'ç‰¹å¾2', 'ç‰¹å¾3']]\ny = df['ç›®æ ‡']\n\n# æ•°æ®åˆ†å‰²\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# æ¨¡å‹è®­ç»ƒ\næ¨¡å‹ = RandomForestClassifier(n_estimators=100, random_state=42)\næ¨¡å‹.fit(X_train, y_train)\n\n# é¢„æµ‹\ny_pred = æ¨¡å‹.predict(X_test)\nprint(classification_report(y_test, y_pred))\n```\n\n## æ·±åº¦å­¦ä¹ \n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# åˆ›å»ºæ¨¡å‹\næ¨¡å‹ = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\n# ç¼–è¯‘\næ¨¡å‹.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# è®­ç»ƒ\nå†å² = æ¨¡å‹.fit(X_train, y_train, epochs=50, validation_split=0.2)\n```\n\n## ç‰¹å¾å·¥ç¨‹\n\n```python\n# åˆ›å»ºæ–°ç‰¹å¾\ndf['ç‰¹å¾æ¯”ç‡'] = df['ç‰¹å¾1'] / df['ç‰¹å¾2']\ndf['ç‰¹å¾æ€»å’Œ'] = df['ç‰¹å¾1'] + df['ç‰¹å¾2']\n\n# å¤šé¡¹å¼ç‰¹å¾\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X_scaled)\n```\n\n## æ¨¡å‹éªŒè¯\n\n```python\nfrom sklearn.model_selection import cross_val_score\n\n# äº¤å‰éªŒè¯\ncv_scores = cross_val_score(æ¨¡å‹, X, y, cv=5, scoring='accuracy')\nprint(f'å¹³å‡CVåˆ†æ•°: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})')\n```\n\n## ç»“è®º\n\næœºå™¨å­¦ä¹ éœ€è¦æ·±å…¥ç†è§£ç®—æ³•ã€æ•°æ®æ“ä½œæŠ€èƒ½å’ŒæŒç»­å­¦ä¹ æ–°æ–¹æ³•ã€‚",
      "isNew": false,
      "isPopular": true,
      "createdAt": "2024-01-01T00:00:00Z"
    },
    {
      "id": 2,
      "title": "æ·±åº¦å­¦ä¹ ï¼šTensorFlowå’ŒPyTorch",
      "description": "æ·±åº¦å­¦ä¹ å®ç”¨æŒ‡å—",
      "category": "technical-tasks",
      "difficulty": "advanced",
      "readTime": 25,
      "rating": 4.8,
      "reads": 987,
      "tags": ["æ·±åº¦å­¦ä¹ ", "TensorFlow", "PyTorch", "ç¥ç»ç½‘ç»œ"],
      "content": "# æ·±åº¦å­¦ä¹ ï¼šTensorFlowå’ŒPyTorch\n\n## TensorFlow\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# å‡½æ•°å¼API\ninputs = tf.keras.Input(shape=(784,))\nx = layers.Dense(128, activation='relu')(inputs)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(10, activation='softmax')(x)\n\næ¨¡å‹ = tf.keras.Model(inputs=inputs, outputs=outputs)\næ¨¡å‹.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n```\n\n## PyTorch\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass ç¥ç»ç½‘ç»œ(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.å±‚1 = nn.Linear(input_size, hidden_size)\n        self.å±‚2 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        x = self.relu(self.å±‚1(x))\n        x = self.å±‚2(x)\n        return x\n\næ¨¡å‹ = ç¥ç»ç½‘ç»œ(784, 128, 10)\næŸå¤±å‡½æ•° = nn.CrossEntropyLoss()\nä¼˜åŒ–å™¨ = torch.optim.Adam(æ¨¡å‹.parameters())\n```\n\n## ç»“è®º\n\næ·±åº¦å­¦ä¹ ä¸ºè§£å†³å¤æ‚é—®é¢˜å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œä½†éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚",
      "isNew": true,
      "isPopular": true,
      "createdAt": "2024-01-15T00:00:00Z"
    },
    {
      "id": 3,
      "title": "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰",
      "description": "ç°ä»£æ–‡æœ¬å¤„ç†å’Œè¯­è¨€æ¨¡å‹æ–¹æ³•",
      "category": "best-practices",
      "difficulty": "advanced",
      "readTime": 22,
      "rating": 4.7,
      "reads": 756,
      "tags": ["NLP", "BERT", "Transformers", "åˆ†è¯"],
      "content": "# è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰\n\n## Transformer\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\næ¨¡å‹åç§° = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(æ¨¡å‹åç§°)\næ¨¡å‹ = AutoModel.from_pretrained(æ¨¡å‹åç§°)\n\n# æ–‡æœ¬åˆ†è¯\næ–‡æœ¬ = \"ä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ\"\ninputs = tokenizer(æ–‡æœ¬, return_tensors=\"pt\", padding=True, truncation=True)\n\n# è·å–åµŒå…¥\nwith torch.no_grad():\n    outputs = æ¨¡å‹(**inputs)\n    embeddings = outputs.last_hidden_state\n```\n\n## å¾®è°ƒ\n\n```python\nfrom transformers import Trainer, TrainingArguments\n\n# è®­ç»ƒå‚æ•°\ntraining_args = TrainingArguments(\n    output_dir='./ç»“æœ',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n)\n\n# è®­ç»ƒ\ntrainer = Trainer(model=æ¨¡å‹, args=training_args, train_dataset=è®­ç»ƒæ•°æ®é›†)\ntrainer.train()\n```\n\n## ç»“è®º\n\nNLPæ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œè¿™è¦å½’åŠŸäºtransformerå’Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚é‡è¦çš„æ˜¯è¦è·Ÿä¸Šæ–°çš„å‘å±•ã€‚",
      "isNew": true,
      "isPopular": false,
      "createdAt": "2024-01-20T00:00:00Z"
    }
  ]
}
