{
  "profession": "data-scientist",
  "language": "fr",
  "categories": [
    {
      "id": "interview-questions",
      "name": "Questions d'Entretien",
      "count": 156,
      "icon": "üí¨",
      "color": "bg-blue-100 text-blue-800"
    },
    {
      "id": "technical-tasks",
      "name": "T√¢ches Techniques",
      "count": 89,
      "icon": "‚ö°",
      "color": "bg-green-100 text-green-800"
    },
    {
      "id": "system-design",
      "name": "Conception de Syst√®mes",
      "count": 45,
      "icon": "üèóÔ∏è",
      "color": "bg-purple-100 text-purple-800"
    },
    {
      "id": "behavioral",
      "name": "Questions Comportementales",
      "count": 67,
      "icon": "üß†",
      "color": "bg-orange-100 text-orange-800"
    },
    {
      "id": "algorithms",
      "name": "Algorithmes & Structures de Donn√©es",
      "count": 123,
      "icon": "üìä",
      "color": "bg-red-100 text-red-800"
    },
    {
      "id": "best-practices",
      "name": "Meilleures Pratiques",
      "count": 78,
      "icon": "‚≠ê",
      "color": "bg-yellow-100 text-yellow-800"
    }
  ],
  "materials": [
    {
      "id": 1,
      "title": "Apprentissage Automatique: Des Bases aux Algorithmes Avanc√©s",
      "description": "Guide complet de l'apprentissage automatique avec des exemples pratiques",
      "category": "interview-questions",
      "difficulty": "advanced",
      "readTime": 30,
      "rating": 4.9,
      "reads": 1345,
      "tags": ["Apprentissage Automatique", "Python", "Scikit-learn", "Algorithmes"],
      "content": "# Apprentissage Automatique: Des Bases aux Algorithmes Avanc√©s\n\n## Apprentissage Supervis√©\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Pr√©paration des donn√©es\nX = df[['caracteristique1', 'caracteristique2', 'caracteristique3']]\ny = df['cible']\n\n# Division des donn√©es\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Entra√Ænement du mod√®le\nmodele = RandomForestClassifier(n_estimators=100, random_state=42)\nmodele.fit(X_train, y_train)\n\n# Pr√©dictions\ny_pred = modele.predict(X_test)\nprint(classification_report(y_test, y_pred))\n```\n\n## Apprentissage Profond\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# Cr√©er le mod√®le\nmodele = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\n# Compiler\nmodele.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Entra√Æner\nhistorique = modele.fit(X_train, y_train, epochs=50, validation_split=0.2)\n```\n\n## Ing√©nierie des Caract√©ristiques\n\n```python\n# Cr√©er de nouvelles caract√©ristiques\ndf['ratio_caracteristique'] = df['caracteristique1'] / df['caracteristique2']\ndf['somme_caracteristique'] = df['caracteristique1'] + df['caracteristique2']\n\n# Caract√©ristiques polynomiales\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X_scaled)\n```\n\n## Validation des Mod√®les\n\n```python\nfrom sklearn.model_selection import cross_val_score\n\n# Validation crois√©e\ncv_scores = cross_val_score(modele, X, y, cv=5, scoring='accuracy')\nprint(f'Score CV moyen: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})')\n```\n\n## Conclusion\n\nL'apprentissage automatique n√©cessite une compr√©hension approfondie des algorithmes, des comp√©tences en manipulation de donn√©es et un apprentissage continu de nouvelles m√©thodes.",
      "isNew": false,
      "isPopular": true,
      "createdAt": "2024-01-01T00:00:00Z"
    },
    {
      "id": 2,
      "title": "Apprentissage Profond avec TensorFlow et PyTorch",
      "description": "Guide pratique de l'apprentissage profond",
      "category": "technical-tasks",
      "difficulty": "advanced",
      "readTime": 25,
      "rating": 4.8,
      "reads": 987,
      "tags": ["Apprentissage Profond", "TensorFlow", "PyTorch", "R√©seaux Neuronaux"],
      "content": "# Apprentissage Profond avec TensorFlow et PyTorch\n\n## TensorFlow\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# API Fonctionnelle\ninputs = tf.keras.Input(shape=(784,))\nx = layers.Dense(128, activation='relu')(inputs)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(10, activation='softmax')(x)\n\nmodele = tf.keras.Model(inputs=inputs, outputs=outputs)\nmodele.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n```\n\n## PyTorch\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass ReseauNeuronal(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.couche1 = nn.Linear(input_size, hidden_size)\n        self.couche2 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        x = self.relu(self.couche1(x))\n        x = self.couche2(x)\n        return x\n\nmodele = ReseauNeuronal(784, 128, 10)\ncriterium = nn.CrossEntropyLoss()\noptimiseur = torch.optim.Adam(modele.parameters())\n```\n\n## Conclusion\n\nL'apprentissage profond ouvre de nouvelles possibilit√©s pour r√©soudre des probl√®mes complexes mais n√©cessite des ressources informatiques importantes.",
      "isNew": true,
      "isPopular": true,
      "createdAt": "2024-01-15T00:00:00Z"
    },
    {
      "id": 3,
      "title": "Traitement du Langage Naturel (NLP)",
      "description": "M√©thodes modernes de traitement de texte et mod√®les de langage",
      "category": "best-practices",
      "difficulty": "advanced",
      "readTime": 22,
      "rating": 4.7,
      "reads": 756,
      "tags": ["NLP", "BERT", "Transformers", "Tokenisation"],
      "content": "# Traitement du Langage Naturel (NLP)\n\n## Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# Charger le mod√®le pr√©-entra√Æn√©\nnom_modele = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(nom_modele)\nmodele = AutoModel.from_pretrained(nom_modele)\n\n# Tokeniser le texte\ntexte = \"Bonjour, comment allez-vous?\"\ninputs = tokenizer(texte, return_tensors=\"pt\", padding=True, truncation=True)\n\n# Obtenir les embeddings\nwith torch.no_grad():\n    outputs = modele(**inputs)\n    embeddings = outputs.last_hidden_state\n```\n\n## Fine-tuning\n\n```python\nfrom transformers import Trainer, TrainingArguments\n\n# Arguments d'entra√Ænement\ntraining_args = TrainingArguments(\n    output_dir='./resultats',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n)\n\n# Entra√Æner\ntrainer = Trainer(model=modele, args=training_args, train_dataset=dataset_entrainement)\ntrainer.train()\n```\n\n## Conclusion\n\nLe NLP √©volue rapidement gr√¢ce aux transformers et aux grands mod√®les de langage. Il est important de se tenir au courant des nouveaux d√©veloppements.",
      "isNew": true,
      "isPopular": false,
      "createdAt": "2024-01-20T00:00:00Z"
    }
  ]
}
