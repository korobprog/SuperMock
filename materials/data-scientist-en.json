{
  "profession": "data-scientist",
  "language": "en",
  "categories": [
    {
      "id": "interview-questions",
      "name": "Interview Questions",
      "count": 156,
      "icon": "üí¨",
      "color": "bg-blue-100 text-blue-800"
    },
    {
      "id": "technical-tasks",
      "name": "Technical Tasks",
      "count": 89,
      "icon": "‚ö°",
      "color": "bg-green-100 text-green-800"
    },
    {
      "id": "system-design",
      "name": "System Design",
      "count": 45,
      "icon": "üèóÔ∏è",
      "color": "bg-purple-100 text-purple-800"
    },
    {
      "id": "behavioral",
      "name": "Behavioral Questions",
      "count": 67,
      "icon": "üß†",
      "color": "bg-orange-100 text-orange-800"
    },
    {
      "id": "algorithms",
      "name": "Algorithms & Data Structures",
      "count": 123,
      "icon": "üìä",
      "color": "bg-red-100 text-red-800"
    },
    {
      "id": "best-practices",
      "name": "Best Practices",
      "count": 78,
      "icon": "‚≠ê",
      "color": "bg-yellow-100 text-yellow-800"
    }
  ],
  "materials": [
    {
      "id": 1,
      "title": "Machine Learning: From Basics to Advanced Algorithms",
      "description": "Comprehensive guide to machine learning with practical examples",
      "category": "interview-questions",
      "difficulty": "advanced",
      "readTime": 30,
      "rating": 4.9,
      "reads": 1345,
      "tags": ["Machine Learning", "Python", "Scikit-learn", "Algorithms"],
      "content": "# Machine Learning: From Basics to Advanced Algorithms\n\n## Supervised Learning\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Data preparation\nX = df[['feature1', 'feature2', 'feature3']]\ny = df['target']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n```\n\n## Deep Learning\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# Create model\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\n# Compile\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train\nhistory = model.fit(X_train, y_train, epochs=50, validation_split=0.2)\n```\n\n## Feature Engineering\n\n```python\n# Create new features\ndf['feature_ratio'] = df['feature1'] / df['feature2']\ndf['feature_sum'] = df['feature1'] + df['feature2']\n\n# Polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X_scaled)\n```\n\n## Model Validation\n\n```python\nfrom sklearn.model_selection import cross_val_score\n\n# Cross-validation\ncv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\nprint(f'Mean CV score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})')\n```\n\n## Conclusion\n\nMachine learning requires deep understanding of algorithms, data manipulation skills, and continuous learning of new methods.",
      "isNew": false,
      "isPopular": true,
      "createdAt": "2024-01-01T00:00:00Z"
    },
    {
      "id": 2,
      "title": "Deep Learning with TensorFlow and PyTorch",
      "description": "Practical guide to deep learning",
      "category": "technical-tasks",
      "difficulty": "advanced",
      "readTime": 25,
      "rating": 4.8,
      "reads": 987,
      "tags": ["Deep Learning", "TensorFlow", "PyTorch", "Neural Networks"],
      "content": "# Deep Learning with TensorFlow and PyTorch\n\n## TensorFlow\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Functional API\ninputs = tf.keras.Input(shape=(784,))\nx = layers.Dense(128, activation='relu')(inputs)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(10, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n```\n\n## PyTorch\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.layer1 = nn.Linear(input_size, hidden_size)\n        self.layer2 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.layer2(x)\n        return x\n\nmodel = NeuralNetwork(784, 128, 10)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\n```\n\n## Conclusion\n\nDeep learning opens new possibilities for solving complex problems but requires significant computational resources.",
      "isNew": true,
      "isPopular": true,
      "createdAt": "2024-01-15T00:00:00Z"
    },
    {
      "id": 3,
      "title": "Natural Language Processing (NLP)",
      "description": "Modern text processing and language models",
      "category": "best-practices",
      "difficulty": "advanced",
      "readTime": 22,
      "rating": 4.7,
      "reads": 756,
      "tags": ["NLP", "BERT", "Transformers", "Tokenization"],
      "content": "# Natural Language Processing (NLP)\n\n## Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# Load pre-trained model\nmodel_name = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n# Tokenize text\ntext = \"Hello, how are you?\"\ninputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n\n# Get embeddings\nwith torch.no_grad():\n    outputs = model(**inputs)\n    embeddings = outputs.last_hidden_state\n```\n\n## Fine-tuning\n\n```python\nfrom transformers import Trainer, TrainingArguments\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n)\n\n# Train\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)\ntrainer.train()\n```\n\n## Conclusion\n\nNLP is rapidly evolving thanks to transformers and large language models. It's important to stay updated with new developments.",
      "isNew": true,
      "isPopular": false,
      "createdAt": "2024-01-20T00:00:00Z"
    }
  ]
}
